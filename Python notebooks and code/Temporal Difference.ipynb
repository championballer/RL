{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(48)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "-96007\n",
      "done\n",
      "\n",
      "36\n",
      "-58430\n",
      "done\n",
      "\n",
      "36\n",
      "-174913\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random policy\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    print(state)\n",
    "    while True:\n",
    "        #print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward+=reward\n",
    "        if done:\n",
    "            print(total_reward)\n",
    "            print(\"done\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(q_state):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(len(q_state)):\n",
    "        if q_state[i]>max:\n",
    "            max = q_state[i]\n",
    "            index = i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env,q_s,episode_i):\n",
    "    \n",
    "    epsilon = 1/episode_i\n",
    "    probs = np.ones(env.action_space.n)*(epsilon/env.action_space.n)\n",
    "    probs[np.argmax(q_s)] = (1-epsilon)+(epsilon/env.action_space.n)\n",
    "    action = np.random.choice(np.arange(env.action_space.n),p=probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,num_episodes,alpha,gamma=1):\n",
    "    \n",
    "    \n",
    "    q = {}\n",
    "    #Initialise q table\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = np.zeros(env.action_space.n)\n",
    "    \n",
    "    for episode_i in range(1,num_episodes+1):\n",
    "        if episode_i % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(episode_i, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        #Initiate the episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        #Select action for initial state using epsilon greedy technique\n",
    "        action = select_action(env,q[state],episode_i)\n",
    "        \n",
    "        #Limiting the number of time steps in an episode\n",
    "        for t_step in range(300):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                q[state][action] = q[state][action]+alpha*(reward-q[state][action])\n",
    "                break\n",
    "            new_action = select_action(env,q[new_state],episode_i)\n",
    "            d_reward = reward+gamma*q[new_state][new_action]\n",
    "            q[state][action] = q[state][action]+alpha*(d_reward-q[state][action])\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000/5000"
     ]
    }
   ],
   "source": [
    "q = sarsa(env,5000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([-10.83352852, -10.83775705, -10.83676747, -10.83982553]),\n",
       " 1: array([-10.36098914, -10.36008403, -10.36510209, -10.36234077]),\n",
       " 2: array([-9.73312883, -9.72146152, -9.72738514, -9.72966008]),\n",
       " 3: array([-9.01531147, -9.01467954, -9.01333794, -9.01374689]),\n",
       " 4: array([-8.26695582, -8.26336939, -8.26906108, -8.27488701]),\n",
       " 5: array([-7.48992921, -7.4909628 , -7.49540621, -7.49335548]),\n",
       " 6: array([-6.72090318, -6.70363689, -6.70519868, -6.71266003]),\n",
       " 7: array([-5.92489337, -5.90740348, -5.90953497, -5.92318005]),\n",
       " 8: array([-5.11055467, -5.11242399, -5.11303452, -5.11630275]),\n",
       " 9: array([-4.33342821, -4.3239973 , -4.32247106, -4.32871794]),\n",
       " 10: array([-3.56609815, -3.55580356, -3.55671498, -3.55324285]),\n",
       " 11: array([-2.84852597, -2.85987167, -2.8499475 , -2.85527388]),\n",
       " 12: array([-11.27544977, -11.27226963, -11.28389281, -11.27553848]),\n",
       " 13: array([-10.61184319, -10.60870046, -10.61523635, -10.6136753 ]),\n",
       " 14: array([-9.84695081, -9.84785618, -9.85161747, -9.85907336]),\n",
       " 15: array([-9.04459787, -9.04473294, -9.04360972, -9.04885615]),\n",
       " 16: array([-8.21754485, -8.21705728, -8.22004494, -8.21714295]),\n",
       " 17: array([-7.37734432, -7.36949937, -7.37414635, -7.38362699]),\n",
       " 18: array([-6.51359133, -6.50968417, -6.50866222, -6.51841849]),\n",
       " 19: array([-5.64272777, -5.63834146, -5.63777377, -5.64897619]),\n",
       " 20: array([-4.76212016, -4.75468709, -4.75581837, -4.7586067 ]),\n",
       " 21: array([-3.86262189, -3.85615871, -3.85554433, -3.87215698]),\n",
       " 22: array([-2.94334714, -2.93964058, -2.94001914, -2.94770539]),\n",
       " 23: array([-2.00743179, -2.01186933, -1.99774711, -2.01436521]),\n",
       " 24: array([-12.00334716, -12.00014461, -12.01114811, -12.00804371]),\n",
       " 25: array([-11.00772935, -11.00006554, -16.09042299, -11.0056628 ]),\n",
       " 26: array([-10.00309732, -10.0000267 , -10.09339379, -10.01970813]),\n",
       " 27: array([ -9.00788945,  -9.00001077, -13.48416437,  -9.0067884 ]),\n",
       " 28: array([-8.01309098, -8.00000478, -8.03295604, -8.01293604]),\n",
       " 29: array([-7.01992234, -7.0000022 , -7.09385221, -7.01876865]),\n",
       " 30: array([-6.00649791, -6.00000087, -8.04063939, -6.02756287]),\n",
       " 31: array([-5.01906487, -5.00000026, -6.08642689, -5.01238025]),\n",
       " 32: array([-4.02003643, -4.00000005, -4.02019548, -4.00594648]),\n",
       " 33: array([-3.02074712, -3.00000001, -5.9229305 , -3.01114484]),\n",
       " 34: array([-2.02175797, -2.        , -2.01648889, -2.00603152]),\n",
       " 35: array([-1.05906898, -1.00571889, -1.        , -1.02877928]),\n",
       " 36: array([-13.00027762, -25.76573172, -13.00994346, -13.00626795]),\n",
       " 37: array([0., 0., 0., 0.]),\n",
       " 38: array([0., 0., 0., 0.]),\n",
       " 39: array([0., 0., 0., 0.]),\n",
       " 40: array([0., 0., 0., 0.]),\n",
       " 41: array([0., 0., 0., 0.]),\n",
       " 42: array([0., 0., 0., 0.]),\n",
       " 43: array([0., 0., 0., 0.]),\n",
       " 44: array([0., 0., 0., 0.]),\n",
       " 45: array([0., 0., 0., 0.]),\n",
       " 46: array([0., 0., 0., 0.]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    \n",
    "    q = {}\n",
    "    #Initialise q table\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = np.zeros(env.action_space.n)\n",
    "    \n",
    "    for episode_i in range(1,num_episodes+1):\n",
    "        \n",
    "        if episode_i%100==0:\n",
    "            print(\"\\rEpisode: {}/{}\".format(episode_i,num_episodes),end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        #Reset state for episode start\n",
    "        state = env.reset()\n",
    "        \n",
    "        #Select initial action for the episode\n",
    "        action = select_action(env,q[state],episode_i)\n",
    "        \n",
    "        #Limiting the length of the episode\n",
    "        for t_step in range(300):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                q[state][action] = q[state][action]+alpha(reward-q[state][action])\n",
    "                break\n",
    "            else:\n",
    "                d_reward = reward + gamma*(np.amax(q[new_state]))\n",
    "                q[state][action] = q[state][action] + alpha*(d_reward-q[state][action])\n",
    "                action = select_action(env,q[new_state],episode_i)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1600/5000"
     ]
    }
   ],
   "source": [
    "q_sarsa_max = q_learning(env,5000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([0., 0., 0., 0.]),\n",
       " 1: array([0., 0., 0., 0.]),\n",
       " 2: array([0., 0., 0., 0.]),\n",
       " 3: array([0., 0., 0., 0.]),\n",
       " 4: array([0., 0., 0., 0.]),\n",
       " 5: array([0., 0., 0., 0.]),\n",
       " 6: array([0., 0., 0., 0.]),\n",
       " 7: array([0., 0., 0., 0.]),\n",
       " 8: array([0., 0., 0., 0.]),\n",
       " 9: array([0., 0., 0., 0.]),\n",
       " 10: array([0., 0., 0., 0.]),\n",
       " 11: array([0., 0., 0., 0.]),\n",
       " 12: array([0., 0., 0., 0.]),\n",
       " 13: array([0., 0., 0., 0.]),\n",
       " 14: array([0., 0., 0., 0.]),\n",
       " 15: array([0., 0., 0., 0.]),\n",
       " 16: array([0., 0., 0., 0.]),\n",
       " 17: array([0., 0., 0., 0.]),\n",
       " 18: array([0., 0., 0., 0.]),\n",
       " 19: array([0., 0., 0., 0.]),\n",
       " 20: array([0., 0., 0., 0.]),\n",
       " 21: array([0., 0., 0., 0.]),\n",
       " 22: array([0., 0., 0., 0.]),\n",
       " 23: array([0., 0., 0., 0.]),\n",
       " 24: array([0., 0., 0., 0.]),\n",
       " 25: array([0., 0., 0., 0.]),\n",
       " 26: array([0., 0., 0., 0.]),\n",
       " 27: array([0., 0., 0., 0.]),\n",
       " 28: array([0., 0., 0., 0.]),\n",
       " 29: array([0., 0., 0., 0.]),\n",
       " 30: array([0., 0., 0., 0.]),\n",
       " 31: array([0., 0., 0., 0.]),\n",
       " 32: array([0., 0., 0., 0.]),\n",
       " 33: array([0., 0., 0., 0.]),\n",
       " 34: array([0., 0., 0., 0.]),\n",
       " 35: array([0., 0., 0., 0.]),\n",
       " 36: array([-3782.26959355, -3782.27484125, -3781.45161535, -3781.4563093 ]),\n",
       " 37: array([0., 0., 0., 0.]),\n",
       " 38: array([0., 0., 0., 0.]),\n",
       " 39: array([0., 0., 0., 0.]),\n",
       " 40: array([0., 0., 0., 0.]),\n",
       " 41: array([0., 0., 0., 0.]),\n",
       " 42: array([0., 0., 0., 0.]),\n",
       " 43: array([0., 0., 0., 0.]),\n",
       " 44: array([0., 0., 0., 0.]),\n",
       " 45: array([0., 0., 0., 0.]),\n",
       " 46: array([0., 0., 0., 0.]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sarsa_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env,num_episodes,alpha,gamma=1):\n",
    "    \n",
    "    q = {}\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
