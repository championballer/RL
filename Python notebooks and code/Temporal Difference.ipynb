{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(48)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "-96007\n",
      "done\n",
      "\n",
      "36\n",
      "-58430\n",
      "done\n",
      "\n",
      "36\n",
      "-174913\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random policy\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    print(state)\n",
    "    while True:\n",
    "        #print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward+=reward\n",
    "        if done:\n",
    "            print(total_reward)\n",
    "            print(\"done\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(q_state):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(len(q_state)):\n",
    "        if q_state[i]>max:\n",
    "            max = q_state[i]\n",
    "            index = i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env,q_s,episode_i):\n",
    "    \n",
    "    epsilon = 1/episode_i\n",
    "    probs = np.ones(env.action_space.n)*(epsilon/env.action_space.n)\n",
    "    probs[np.argmax(q_s)] = (1-epsilon)+(epsilon/env.action_space.n)\n",
    "    action = np.random.choice(np.arange(env.action_space.n),p=probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,num_episodes,alpha,gamma=1):\n",
    "    \n",
    "    \n",
    "    q = {}\n",
    "    #Initialise q table\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = np.zeros(env.action_space.n)\n",
    "    \n",
    "    for episode_i in range(1,num_episodes+1):\n",
    "        if episode_i % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(episode_i, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        #Initiate the episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        #Select action for initial state using epsilon greedy technique\n",
    "        action = select_action(env,q[state],episode_i)\n",
    "        \n",
    "        #Limiting the number of time steps in an episode\n",
    "        for t_step in range(300):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                q[state][action] = q[state][action]+alpha*(reward-q[state][action])\n",
    "                break\n",
    "            new_action = select_action(env,q[new_state],episode_i)\n",
    "            d_reward = reward+gamma*q[new_state][new_action]\n",
    "            q[state][action] = q[state][action]+alpha*(d_reward-q[state][action])\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000/5000"
     ]
    }
   ],
   "source": [
    "q = sarsa(env,5000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([-10.85372504, -10.84408028, -10.83938087, -10.85024928]),\n",
       " 1: array([-10.37033166, -10.36460965, -10.36538761, -10.37351232]),\n",
       " 2: array([-9.74230081, -9.72583345, -9.73042918, -9.73824682]),\n",
       " 3: array([-9.02495462, -9.01611296, -9.02080419, -9.0188189 ]),\n",
       " 4: array([-8.27768661, -8.26704303, -8.26674348, -8.2754646 ]),\n",
       " 5: array([-7.50961746, -7.49392401, -7.49406791, -7.49971065]),\n",
       " 6: array([-6.71252954, -6.70539527, -6.70824613, -6.71878319]),\n",
       " 7: array([-5.91523533, -5.91076341, -5.91438519, -5.91616394]),\n",
       " 8: array([-5.11958623, -5.11526759, -5.11388636, -5.1274691 ]),\n",
       " 9: array([-4.34285577, -4.32661934, -4.32463795, -4.3347765 ]),\n",
       " 10: array([-3.55619177, -3.55890233, -3.55938789, -3.56337461]),\n",
       " 11: array([-2.86831459, -2.86999461, -2.85178025, -2.85494346]),\n",
       " 12: array([-11.27862475, -11.27409246, -11.28363429, -11.28545689]),\n",
       " 13: array([-10.61318114, -10.60875763, -10.61673427, -10.62264871]),\n",
       " 14: array([-9.85278422, -9.84828221, -9.8480089 , -9.86128294]),\n",
       " 15: array([-9.04533024, -9.04636892, -9.04963285, -9.04498417]),\n",
       " 16: array([-8.22189001, -8.21573298, -8.21986915, -8.22669214]),\n",
       " 17: array([-7.37268944, -7.36890785, -7.37440889, -7.36821745]),\n",
       " 18: array([-6.52012049, -6.50944016, -6.51094132, -6.51550325]),\n",
       " 19: array([-5.64668325, -5.6375584 , -5.64092599, -5.63936835]),\n",
       " 20: array([-4.76163135, -4.75462212, -4.7539409 , -4.75793395]),\n",
       " 21: array([-3.85933881, -3.85864369, -3.85920756, -3.85867776]),\n",
       " 22: array([-2.95722683, -2.94547374, -2.945997  , -2.95650103]),\n",
       " 23: array([-2.0075856 , -2.01190178, -1.9981046 , -2.01283518]),\n",
       " 24: array([-12.00425055, -12.00078342, -12.03404012, -12.0161358 ]),\n",
       " 25: array([-11.00805063, -11.00048356, -11.6590761 , -11.0028018 ]),\n",
       " 26: array([-10.00692399, -10.00026214, -15.89701314, -10.0264869 ]),\n",
       " 27: array([ -9.00803475,  -9.00012879, -10.67512018,  -9.04046499]),\n",
       " 28: array([-8.01509015, -8.00006108, -8.01033269, -8.02322999]),\n",
       " 29: array([-7.01164139, -7.00002894, -8.21332816, -7.01429367]),\n",
       " 30: array([-6.0210569 , -6.00001271, -6.01467808, -6.02357119]),\n",
       " 31: array([-5.00840867, -5.00000448, -7.11973612, -5.01914606]),\n",
       " 32: array([-4.00101698, -4.00000108, -5.02146005, -4.01830716]),\n",
       " 33: array([-3.02041675, -3.00000013, -4.05207516, -3.00340624]),\n",
       " 34: array([-2.07044875, -2.        , -5.18382979, -2.01797462]),\n",
       " 35: array([-1.03580038, -1.04493231, -1.        , -1.06947576]),\n",
       " 36: array([-13.00117687, -33.2541411 , -13.00267887, -13.00923144]),\n",
       " 37: array([0., 0., 0., 0.]),\n",
       " 38: array([0., 0., 0., 0.]),\n",
       " 39: array([0., 0., 0., 0.]),\n",
       " 40: array([0., 0., 0., 0.]),\n",
       " 41: array([0., 0., 0., 0.]),\n",
       " 42: array([0., 0., 0., 0.]),\n",
       " 43: array([0., 0., 0., 0.]),\n",
       " 44: array([0., 0., 0., 0.]),\n",
       " 45: array([0., 0., 0., 0.]),\n",
       " 46: array([0., 0., 0., 0.]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    \n",
    "    q = {}\n",
    "    #Initialise q table\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = np.zeros(env.action_space.n)\n",
    "    \n",
    "    for episode_i in range(1,num_episodes+1):\n",
    "        \n",
    "        if episode_i%100==0:\n",
    "            print(\"\\rEpisode: {}/{}\".format(episode_i,num_episodes),end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        #Reset state for episode start\n",
    "        state = env.reset()\n",
    "        \n",
    "        #Select initial action for the episode\n",
    "        action = select_action(env,q[state],episode_i)\n",
    "        \n",
    "        #Limiting the length of the episode\n",
    "        for t_step in range(300):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                q[state][action] = q[state][action]+alpha*(reward-q[state][action])\n",
    "                break\n",
    "            else:\n",
    "                d_reward = reward + gamma*(np.amax(q[new_state]))\n",
    "                q[state][action] = q[state][action] + alpha*(d_reward-q[state][action])\n",
    "                action = select_action(env,q[new_state],episode_i)\n",
    "                state = new_state\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5000/5000"
     ]
    }
   ],
   "source": [
    "q_sarsa_max = q_learning(env,5000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([-10.83845175, -10.83562333, -10.84255784, -10.83925423]),\n",
       " 1: array([-10.3589916 , -10.35911895, -10.36467582, -10.36164715]),\n",
       " 2: array([-9.72930908, -9.72288439, -9.72509914, -9.73568694]),\n",
       " 3: array([-9.01882019, -9.01443372, -9.01915413, -9.01737972]),\n",
       " 4: array([-8.26951944, -8.26460198, -8.26915433, -8.26505527]),\n",
       " 5: array([-7.4897498 , -7.49099231, -7.49365883, -7.49820304]),\n",
       " 6: array([-6.70820043, -6.70289107, -6.70381942, -6.71406212]),\n",
       " 7: array([-5.90956328, -5.90747798, -5.90959139, -5.92093037]),\n",
       " 8: array([-5.10982893, -5.11092051, -5.11571557, -5.1256182 ]),\n",
       " 9: array([-4.32985196, -4.32246876, -4.3246565 , -4.33979857]),\n",
       " 10: array([-3.55980713, -3.55642398, -3.55419035, -3.56437302]),\n",
       " 11: array([-2.84975699, -2.84962079, -2.84915289, -2.86181069]),\n",
       " 12: array([-11.27568654, -11.27090537, -11.27280119, -11.27869397]),\n",
       " 13: array([-10.60918142, -10.60840264, -10.61016897, -10.60614191]),\n",
       " 14: array([-9.85259423, -9.84811792, -9.84689854, -9.85486556]),\n",
       " 15: array([-9.05181097, -9.04358378, -9.0453669 , -9.05908017]),\n",
       " 16: array([-8.21506677, -8.21374834, -8.21768754, -8.22944385]),\n",
       " 17: array([-7.37074779, -7.36837493, -7.3695568 , -7.3778639 ]),\n",
       " 18: array([-6.50860279, -6.5088661 , -6.51290619, -6.52099822]),\n",
       " 19: array([-5.63852141, -5.63724128, -5.63951415, -5.65381733]),\n",
       " 20: array([-4.75500162, -4.75412479, -4.75458074, -4.76880905]),\n",
       " 21: array([-3.86920929, -3.85541799, -3.85584723, -3.86463145]),\n",
       " 22: array([-2.95085124, -2.938991  , -2.93870811, -2.95536064]),\n",
       " 23: array([-1.99939271, -1.99990193, -1.99765322, -2.01591443]),\n",
       " 24: array([-12.00055261, -11.99999981, -12.00346678, -12.0071648 ]),\n",
       " 25: array([-11.00341648, -10.99999996, -15.01278288, -11.02904176]),\n",
       " 26: array([-10.00199963,  -9.99999999, -11.51206309, -10.00776096]),\n",
       " 27: array([-9.00375801, -9.        , -9.87398005, -9.007363  ]),\n",
       " 28: array([-8.00000228, -8.        , -8.84302027, -8.01523189]),\n",
       " 29: array([-7.01206945, -7.        , -7.02580194, -7.00388404]),\n",
       " 30: array([-6.01376521, -6.        , -6.1109341 , -6.01803926]),\n",
       " 31: array([-5.00981198, -5.        , -6.08495404, -5.01972161]),\n",
       " 32: array([-4.02857186, -4.        , -4.06732992, -4.01974409]),\n",
       " 33: array([-3.00602332, -3.        , -3.04625185, -3.01014832]),\n",
       " 34: array([-2.00660962, -2.        , -3.11876989, -2.00058049]),\n",
       " 35: array([-1.03202343, -1.00986867, -1.        , -1.01504023]),\n",
       " 36: array([-12.9999992 , -32.01284871, -13.00693839, -13.00546308]),\n",
       " 37: array([0., 0., 0., 0.]),\n",
       " 38: array([0., 0., 0., 0.]),\n",
       " 39: array([0., 0., 0., 0.]),\n",
       " 40: array([0., 0., 0., 0.]),\n",
       " 41: array([0., 0., 0., 0.]),\n",
       " 42: array([0., 0., 0., 0.]),\n",
       " 43: array([0., 0., 0., 0.]),\n",
       " 44: array([0., 0., 0., 0.]),\n",
       " 45: array([0., 0., 0., 0.]),\n",
       " 46: array([0., 0., 0., 0.]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sarsa_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cum_reward(env,q_s,episode_i):\n",
    "    \n",
    "    epsilon = 1/episode_i\n",
    "    \n",
    "    probs = np.ones(env.nA)*(epsilon/env.nA)\n",
    "    probs[np.argmax(q_s)] = (1-epsilon)+(epsilon/env.nA)\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(len(q_s)):\n",
    "        sum+=(probs[i]*q_s[i])\n",
    "        \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env,num_episodes,alpha,gamma=1):\n",
    "    \n",
    "    #Initialise the Q table\n",
    "    q = {}\n",
    "    for i in range(env.observation_space.n):\n",
    "        q[i] = np.zeros(env.nA)\n",
    "    \n",
    "    for episode_i in range(1,num_episodes+1):\n",
    "        \n",
    "        if episode_i%100==0:\n",
    "            print(\"\\rEpisode: {}/{}\".format(episode_i,num_episodes),end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        #reset environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        #choose init action\n",
    "        action = select_action(env,q[state],episode_i)\n",
    "        \n",
    "        for t_step in range(300):\n",
    "            new_state,reward,done,info = env.step(action)\n",
    "            \n",
    "            if not done:\n",
    "                d_reward = reward + find_cum_reward(env,q[new_state],episode_i)\n",
    "                q[state][action] = q[state][action]+alpha*(d_reward-q[state][action])\n",
    "                action = select_action(env,q[new_state],episode_i)\n",
    "                state = new_state\n",
    "            else:\n",
    "                q[state][action] = q[state][action]+alpha*(reward-q[state][action])\n",
    "                break\n",
    "                \n",
    "    return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5000/5000"
     ]
    }
   ],
   "source": [
    "q_exsarsa = expected_sarsa(env,5000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([-10.84043556, -10.83846808, -10.84463802, -10.83919164]),\n",
       " 1: array([-10.35782744, -10.35633396, -10.3673726 , -10.36778016]),\n",
       " 2: array([-9.72009056, -9.72233838, -9.72784496, -9.72714661]),\n",
       " 3: array([-9.01911095, -9.01183234, -9.01878301, -9.02593526]),\n",
       " 4: array([-8.26020774, -8.26163985, -8.26833696, -8.2733428 ]),\n",
       " 5: array([-7.49028031, -7.48735425, -7.49208925, -7.49737295]),\n",
       " 6: array([-6.70050744, -6.69947219, -6.70599093, -6.69875008]),\n",
       " 7: array([-5.91019093, -5.90554448, -5.90886767, -5.91137288]),\n",
       " 8: array([-5.11020708, -5.10852857, -5.1140984 , -5.12485635]),\n",
       " 9: array([-4.31998023, -4.31934426, -4.32260805, -4.32690499]),\n",
       " 10: array([-3.55995129, -3.55279317, -3.55451778, -3.55193578]),\n",
       " 11: array([-2.85014778, -2.84981728, -2.84849153, -2.85106004]),\n",
       " 12: array([-11.27661797, -11.27188458, -11.28682457, -11.27903792]),\n",
       " 13: array([-10.60789116, -10.60758399, -10.60902561, -10.62115936]),\n",
       " 14: array([-9.85071751, -9.84861704, -9.84803553, -9.84776022]),\n",
       " 15: array([-9.04262874, -9.04432807, -9.05146958, -9.04945479]),\n",
       " 16: array([-8.21636527, -8.21580538, -8.21470052, -8.21802023]),\n",
       " 17: array([-7.3724943 , -7.36901823, -7.37080062, -7.3772132 ]),\n",
       " 18: array([-6.50883097, -6.50969803, -6.51062614, -6.52255331]),\n",
       " 19: array([-5.64042024, -5.63856355, -5.6396009 , -5.64667368]),\n",
       " 20: array([-4.75903898, -4.75520748, -4.75603748, -4.76442014]),\n",
       " 21: array([-3.85973152, -3.85548899, -3.85639285, -3.86985982]),\n",
       " 22: array([-2.9519914 , -2.9390306 , -2.93907752, -2.94033405]),\n",
       " 23: array([-2.00389534, -2.00005144, -1.99761737, -2.01262071]),\n",
       " 24: array([-12.0011225 , -12.00076172, -12.01953009, -12.0059024 ]),\n",
       " 25: array([-11.00422778, -11.00057893, -14.29837992, -11.01408278]),\n",
       " 26: array([-10.00448533, -10.00051609, -10.97023168, -10.02185531]),\n",
       " 27: array([-9.00174761, -9.00045315, -9.9923249 , -9.03561433]),\n",
       " 28: array([-8.00277815, -8.0003487 , -9.86980212, -8.01816372]),\n",
       " 29: array([-7.00392103, -7.00033311, -7.14725306, -7.00844758]),\n",
       " 30: array([-6.01028389, -6.00032238, -6.042665  , -6.01923386]),\n",
       " 31: array([-5.02442437, -5.00021347, -6.97956597, -5.03964078]),\n",
       " 32: array([-4.03294166, -4.00015805, -4.96707947, -4.01986141]),\n",
       " 33: array([-3.00399337, -3.00005578, -4.96699319, -3.00988636]),\n",
       " 34: array([-2.00776804, -2.00000276, -3.01807892, -2.01205664]),\n",
       " 35: array([-1.0014029 , -1.00998227, -1.        , -1.04264165]),\n",
       " 36: array([-13.00078041, -25.79170014, -13.01017713, -13.00464086]),\n",
       " 37: array([0., 0., 0., 0.]),\n",
       " 38: array([0., 0., 0., 0.]),\n",
       " 39: array([0., 0., 0., 0.]),\n",
       " 40: array([0., 0., 0., 0.]),\n",
       " 41: array([0., 0., 0., 0.]),\n",
       " 42: array([0., 0., 0., 0.]),\n",
       " 43: array([0., 0., 0., 0.]),\n",
       " 44: array([0., 0., 0., 0.]),\n",
       " 45: array([0., 0., 0., 0.]),\n",
       " 46: array([0., 0., 0., 0.]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_exsarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
