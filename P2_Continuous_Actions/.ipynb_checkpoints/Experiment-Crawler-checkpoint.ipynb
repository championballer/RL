{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name='Crawler.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.elu(self.fc1(state))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.elu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4 #3e-5 #1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4 #3e-5 #1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY_actor = 0.0 #3e-4 #0        # L2 weight decay\n",
    "WEIGHT_DECAY_critic = 0.0 #1e-6 #0        # L2 weight decay\n",
    "\n",
    "#to decay exploration as it learns\n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY_critic)\n",
    "\n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed) #single agent only\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed) #both singe and multiple agent\n",
    "        self.eps = EPS_START\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "        # Make sure target is initialized with the same weight as the source (found on slack to make big difference)\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        #Experience from each agent is separately saved (so it works for single or multi agent environment)\n",
    "        #This works because each agent is operating in a separate/independent environment\n",
    "        for a in range(self.num_agents):\n",
    "            self.memory.add(states[a], actions[a], rewards[a], next_states[a], dones[a])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        #add noise according to epsilon probability\n",
    "        if add_noise and (np.random.random() < self.eps):\n",
    "            actions += self.noise.sample()\n",
    "            #update the exploration parameter\n",
    "            self.eps -= EPS_DECAY\n",
    "            if self.eps < EPS_END:\n",
    "                self.eps = EPS_END\n",
    "            #self.noise.reset() #not sure if need to do this here\n",
    "\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0) #clip the gradient for the critic network (Udacity hint)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "    ## from slack - Since you're using DDPG, @gregoriomezquita mentioned that \n",
    "    ## initializing the weights of the target networks to be the same as those \n",
    "    ## of the live networks seemed to make a huge difference\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(source_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], action_size=brain.vector_action_space_size, \n",
    "              num_agents=env_info.vector_observations.shape[0],  random_seed=0)\n",
    "\n",
    "\n",
    "#for single and multiple agents\n",
    "def ddpg(n_episodes=2000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_list = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                   # get the current states (for all agents)\n",
    "        agent.reset() #reset the agent OU Noise\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            agent.step(states, actions, rewards, next_states, dones) #train the agent\n",
    "            \n",
    "            # Extra Learning per time step (since generating so much experience at each step)\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                for _ in range(3):\n",
    "                    experiences = agent.memory.sample()\n",
    "                    agent.learn(experiences, GAMMA)\n",
    "            \n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            #print('Total score (averaged over agents) this episode: {}'.format(np.mean(score)))\n",
    "        \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        scores_list.append(np.mean(scores))\n",
    "        \n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.mean(scores)))\n",
    "        print('Epsilon: {} and Memory size: {}'.format(agent.eps, len(agent.memory)))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) > 30 and len(scores_deque) >= 100:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.16\tScore: 0.15652234161583087\n",
      "Epsilon: 0.9997600000000002 and Memory size: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/openai/lib/python3.6/site-packages/ipykernel_launcher.py:117: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\tAverage Score: 0.21\tScore: 0.27196187917919207\n",
      "Epsilon: 0.9995200000000004 and Memory size: 192\n",
      "Episode 3\tAverage Score: 0.25\tScore: 0.308707063474382\n",
      "Epsilon: 0.9992800000000006 and Memory size: 288\n",
      "Episode 4\tAverage Score: 0.16\tScore: -0.10914183101461579\n",
      "Epsilon: 0.9989200000000009 and Memory size: 432\n",
      "Episode 5\tAverage Score: 0.17\tScore: 0.2265078913187608\n",
      "Epsilon: 0.9986800000000011 and Memory size: 528\n",
      "Episode 6\tAverage Score: 0.17\tScore: 0.19299482825833061\n",
      "Epsilon: 0.9984400000000013 and Memory size: 624\n",
      "Episode 7\tAverage Score: 0.20\tScore: 0.36439737856077653\n",
      "Epsilon: 0.9981700000000016 and Memory size: 732\n",
      "Episode 8\tAverage Score: 0.22\tScore: 0.3568820108970006\n",
      "Epsilon: 0.9979300000000018 and Memory size: 828\n",
      "Episode 9\tAverage Score: 0.18\tScore: -0.13170323140608767\n",
      "Epsilon: 0.997660000000002 and Memory size: 936\n",
      "Episode 10\tAverage Score: 0.15\tScore: -0.12398545195659001\n",
      "Epsilon: 0.9974200000000022 and Memory size: 1032\n",
      "Episode 11\tAverage Score: 0.12\tScore: -0.14626443944871426\n",
      "Epsilon: 0.9971200000000024 and Memory size: 1152\n",
      "Episode 12\tAverage Score: 0.13\tScore: 0.20464548119343817\n",
      "Epsilon: 0.9969100000000026 and Memory size: 1236\n",
      "Episode 13\tAverage Score: 0.10\tScore: -0.28726328695969033\n",
      "Epsilon: 0.9966700000000028 and Memory size: 1332\n",
      "Episode 14\tAverage Score: 0.07\tScore: -0.2966612159507349\n",
      "Epsilon: 0.996430000000003 and Memory size: 1428\n",
      "Episode 15\tAverage Score: 0.06\tScore: -0.03995473519898951\n",
      "Epsilon: 0.9961900000000032 and Memory size: 1524\n",
      "Episode 16\tAverage Score: 0.06\tScore: -0.01006645030186822\n",
      "Epsilon: 0.9959500000000034 and Memory size: 1620\n",
      "Episode 17\tAverage Score: 0.04\tScore: -0.17320884725389382\n",
      "Epsilon: 0.9957400000000036 and Memory size: 1716\n",
      "Episode 18\tAverage Score: 0.04\tScore: 0.02409364131744951\n",
      "Epsilon: 0.9955000000000038 and Memory size: 1812\n",
      "Episode 19\tAverage Score: 0.05\tScore: 0.07227113524762292\n",
      "Epsilon: 0.995290000000004 and Memory size: 1908\n",
      "Episode 20\tAverage Score: 0.05\tScore: 0.095918927846166\n",
      "Epsilon: 0.9950500000000042 and Memory size: 2004\n",
      "Episode 21\tAverage Score: 0.05\tScore: 0.0035001469465593496\n",
      "Epsilon: 0.9948100000000044 and Memory size: 2100\n",
      "Episode 22\tAverage Score: 0.06\tScore: 0.40457809165430564\n",
      "Epsilon: 0.9946000000000046 and Memory size: 2196\n",
      "Episode 23\tAverage Score: 0.07\tScore: 0.14109734857144454\n",
      "Epsilon: 0.9943600000000048 and Memory size: 2292\n",
      "Episode 24\tAverage Score: 0.08\tScore: 0.4639918044364701\n",
      "Epsilon: 0.994150000000005 and Memory size: 2376\n",
      "Episode 25\tAverage Score: 0.07\tScore: -0.11997855035588145\n",
      "Epsilon: 0.9939100000000052 and Memory size: 2472\n",
      "Episode 26\tAverage Score: 0.07\tScore: -0.1136099911139657\n",
      "Epsilon: 0.9936700000000054 and Memory size: 2568\n",
      "Episode 27\tAverage Score: 0.06\tScore: -0.04342786082997918\n",
      "Epsilon: 0.9934300000000056 and Memory size: 2664\n",
      "Episode 28\tAverage Score: 0.06\tScore: -0.12090276457214107\n",
      "Epsilon: 0.9931900000000058 and Memory size: 2760\n",
      "Episode 29\tAverage Score: 0.06\tScore: 0.11208258126862347\n",
      "Epsilon: 0.992950000000006 and Memory size: 2856\n",
      "Episode 30\tAverage Score: 0.05\tScore: -0.04271449722970525\n",
      "Epsilon: 0.9927100000000062 and Memory size: 2952\n",
      "Episode 31\tAverage Score: 0.05\tScore: -0.1753128730536749\n",
      "Epsilon: 0.9924700000000064 and Memory size: 3048\n",
      "Episode 32\tAverage Score: 0.04\tScore: -0.19374980280796686\n",
      "Epsilon: 0.9922300000000066 and Memory size: 3144\n",
      "Episode 33\tAverage Score: 0.04\tScore: -0.08244755922351032\n",
      "Epsilon: 0.9919900000000068 and Memory size: 3240\n",
      "Episode 34\tAverage Score: 0.03\tScore: -0.23398219669858614\n",
      "Epsilon: 0.991750000000007 and Memory size: 3336\n",
      "Episode 35\tAverage Score: 0.02\tScore: -0.0842389053820322\n",
      "Epsilon: 0.9915400000000072 and Memory size: 3432\n",
      "Episode 36\tAverage Score: 0.02\tScore: -0.19337123857500652\n",
      "Epsilon: 0.9913000000000074 and Memory size: 3528\n",
      "Episode 37\tAverage Score: 0.01\tScore: -0.192397986461098\n",
      "Epsilon: 0.9910600000000076 and Memory size: 3624\n",
      "Episode 38\tAverage Score: 0.01\tScore: -0.2868520263582468\n",
      "Epsilon: 0.9908200000000078 and Memory size: 3720\n",
      "Episode 39\tAverage Score: -0.00\tScore: -0.2614736055256799\n",
      "Epsilon: 0.990580000000008 and Memory size: 3816\n",
      "Episode 40\tAverage Score: -0.01\tScore: -0.15903480032769343\n",
      "Epsilon: 0.9903400000000082 and Memory size: 3912\n",
      "Episode 41\tAverage Score: -0.01\tScore: -0.2273683223562936\n",
      "Epsilon: 0.9901300000000084 and Memory size: 4008\n",
      "Episode 42\tAverage Score: -0.01\tScore: -0.1673067711138477\n",
      "Epsilon: 0.9898900000000086 and Memory size: 4104\n",
      "Episode 43\tAverage Score: -0.02\tScore: -0.08659504373402645\n",
      "Epsilon: 0.9896500000000088 and Memory size: 4200\n",
      "Episode 44\tAverage Score: -0.02\tScore: -0.25583044656862813\n",
      "Epsilon: 0.989410000000009 and Memory size: 4296\n",
      "Episode 45\tAverage Score: -0.01\tScore: 0.4035496877816816\n",
      "Epsilon: 0.9892000000000092 and Memory size: 4380\n",
      "Episode 46\tAverage Score: -0.00\tScore: 0.3806958739102508\n",
      "Epsilon: 0.9889900000000094 and Memory size: 4464\n",
      "Episode 47\tAverage Score: -0.00\tScore: -0.024238571136568982\n",
      "Epsilon: 0.9887500000000096 and Memory size: 4560\n",
      "Episode 48\tAverage Score: -0.01\tScore: -0.1872718477000793\n",
      "Epsilon: 0.9885100000000098 and Memory size: 4656\n",
      "Episode 49\tAverage Score: -0.01\tScore: -0.04991705354768783\n",
      "Epsilon: 0.98827000000001 and Memory size: 4752\n",
      "Episode 50\tAverage Score: -0.01\tScore: -0.07409306584546964\n",
      "Epsilon: 0.9880300000000102 and Memory size: 4848\n",
      "Episode 51\tAverage Score: -0.01\tScore: 0.0729402635867397\n",
      "Epsilon: 0.9877900000000104 and Memory size: 4944\n",
      "Episode 52\tAverage Score: -0.01\tScore: -0.11462742245445649\n",
      "Epsilon: 0.9875500000000106 and Memory size: 5040\n",
      "Episode 53\tAverage Score: -0.01\tScore: 0.043283330354218684\n",
      "Epsilon: 0.9873100000000108 and Memory size: 5136\n",
      "Episode 54\tAverage Score: -0.01\tScore: -0.14651930045026043\n",
      "Epsilon: 0.987070000000011 and Memory size: 5232\n",
      "Episode 55\tAverage Score: -0.01\tScore: -0.047284731719022\n",
      "Epsilon: 0.9868600000000112 and Memory size: 5328\n",
      "Episode 56\tAverage Score: -0.02\tScore: -0.17214475905833146\n",
      "Epsilon: 0.9866200000000114 and Memory size: 5424\n",
      "Episode 57\tAverage Score: -0.02\tScore: -0.14959218834216395\n",
      "Epsilon: 0.9863800000000116 and Memory size: 5520\n",
      "Episode 58\tAverage Score: -0.02\tScore: -0.16427685537685952\n",
      "Epsilon: 0.9861400000000118 and Memory size: 5616\n",
      "Episode 59\tAverage Score: -0.02\tScore: 0.2021607837329308\n",
      "Epsilon: 0.985900000000012 and Memory size: 5712\n",
      "Episode 60\tAverage Score: -0.01\tScore: 0.1890729715426763\n",
      "Epsilon: 0.9856600000000122 and Memory size: 5808\n",
      "Episode 61\tAverage Score: -0.01\tScore: 0.4084163581331571\n",
      "Epsilon: 0.9854200000000124 and Memory size: 5904\n",
      "Episode 62\tAverage Score: -0.01\tScore: 0.04907021904364228\n",
      "Epsilon: 0.9851500000000126 and Memory size: 6012\n",
      "Episode 63\tAverage Score: 0.00\tScore: 0.3824633188390483\n",
      "Epsilon: 0.9849400000000128 and Memory size: 6108\n",
      "Episode 64\tAverage Score: 0.00\tScore: 0.25401871422460925\n",
      "Epsilon: 0.984670000000013 and Memory size: 6216\n",
      "Episode 65\tAverage Score: 0.01\tScore: 0.11498641754345347\n",
      "Epsilon: 0.9844300000000132 and Memory size: 6312\n",
      "Episode 66\tAverage Score: 0.01\tScore: 0.2675037210962425\n",
      "Epsilon: 0.9841900000000134 and Memory size: 6408\n",
      "Episode 67\tAverage Score: 0.02\tScore: 0.40609025695205975\n",
      "Epsilon: 0.9839800000000136 and Memory size: 6504\n",
      "Episode 68\tAverage Score: 0.02\tScore: 0.5481736649138232\n",
      "Epsilon: 0.9837400000000138 and Memory size: 6600\n",
      "Episode 69\tAverage Score: 0.03\tScore: 0.4774715950479731\n",
      "Epsilon: 0.983500000000014 and Memory size: 6696\n",
      "Episode 70\tAverage Score: 0.04\tScore: 0.4722225175161536\n",
      "Epsilon: 0.9832600000000142 and Memory size: 6792\n",
      "Episode 71\tAverage Score: 0.04\tScore: 0.47748194021793705\n",
      "Epsilon: 0.9829900000000145 and Memory size: 6900\n",
      "Episode 72\tAverage Score: 0.05\tScore: 0.47010589751880616\n",
      "Epsilon: 0.9827500000000147 and Memory size: 7008\n",
      "Episode 73\tAverage Score: 0.06\tScore: 0.5304441129943976\n",
      "Epsilon: 0.9825400000000148 and Memory size: 7104\n",
      "Episode 74\tAverage Score: 0.06\tScore: 0.5196408596821129\n",
      "Epsilon: 0.982300000000015 and Memory size: 7200\n",
      "Episode 75\tAverage Score: 0.06\tScore: 0.28952062351163477\n",
      "Epsilon: 0.9820600000000153 and Memory size: 7296\n",
      "Episode 76\tAverage Score: 0.07\tScore: 0.40246848249807954\n",
      "Epsilon: 0.9817000000000156 and Memory size: 7440\n",
      "Episode 77\tAverage Score: 0.08\tScore: 0.5553620714539041\n",
      "Epsilon: 0.9814600000000158 and Memory size: 7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78\tAverage Score: 0.08\tScore: 0.4581594908765207\n",
      "Epsilon: 0.981220000000016 and Memory size: 7644\n",
      "Episode 79\tAverage Score: 0.08\tScore: 0.37095152811768156\n",
      "Epsilon: 0.9809500000000162 and Memory size: 7752\n",
      "Episode 80\tAverage Score: 0.09\tScore: 0.37633257801644504\n",
      "Epsilon: 0.9806800000000164 and Memory size: 7860\n",
      "Episode 81\tAverage Score: 0.09\tScore: 0.5318349925801158\n",
      "Epsilon: 0.9804400000000166 and Memory size: 7956\n",
      "Episode 82\tAverage Score: 0.10\tScore: 0.44998363537403446\n",
      "Epsilon: 0.9802000000000168 and Memory size: 8052\n",
      "Episode 83\tAverage Score: 0.10\tScore: 0.39252062801582116\n",
      "Epsilon: 0.979960000000017 and Memory size: 8148\n",
      "Episode 84\tAverage Score: 0.11\tScore: 0.47529388847760856\n",
      "Epsilon: 0.9796900000000173 and Memory size: 8256\n",
      "Episode 85\tAverage Score: 0.11\tScore: 0.35025351129782695\n",
      "Epsilon: 0.9794500000000175 and Memory size: 8352\n",
      "Episode 86\tAverage Score: 0.11\tScore: 0.5310959264946481\n",
      "Epsilon: 0.9792100000000177 and Memory size: 8448\n",
      "Episode 87\tAverage Score: 0.12\tScore: 0.3697763937525451\n",
      "Epsilon: 0.9789400000000179 and Memory size: 8556\n",
      "Episode 88\tAverage Score: 0.12\tScore: 0.5289753343289098\n",
      "Epsilon: 0.9787000000000181 and Memory size: 8652\n",
      "Episode 89\tAverage Score: 0.12\tScore: 0.3829154477377112\n",
      "Epsilon: 0.9784300000000183 and Memory size: 8760\n",
      "Episode 90\tAverage Score: 0.13\tScore: 0.42884116631466895\n",
      "Epsilon: 0.9781900000000185 and Memory size: 8856\n",
      "Episode 91\tAverage Score: 0.13\tScore: 0.4510221368012329\n",
      "Epsilon: 0.9779500000000187 and Memory size: 8952\n",
      "Episode 92\tAverage Score: 0.13\tScore: 0.30267965987635154\n",
      "Epsilon: 0.977680000000019 and Memory size: 9072\n",
      "Episode 93\tAverage Score: 0.14\tScore: 0.3764340537600219\n",
      "Epsilon: 0.9774400000000192 and Memory size: 9168\n",
      "Episode 94\tAverage Score: 0.14\tScore: 0.518000559338058\n",
      "Epsilon: 0.9772000000000194 and Memory size: 9264\n",
      "Episode 95\tAverage Score: 0.14\tScore: 0.3333950792827333\n",
      "Epsilon: 0.9770500000000195 and Memory size: 9360\n",
      "Episode 96\tAverage Score: 0.15\tScore: 0.5502259211304287\n",
      "Epsilon: 0.9767800000000197 and Memory size: 9468\n",
      "Episode 97\tAverage Score: 0.15\tScore: 0.46532431508724886\n",
      "Epsilon: 0.97654000000002 and Memory size: 9564\n",
      "Episode 98\tAverage Score: 0.15\tScore: 0.49371203089443344\n",
      "Epsilon: 0.9763300000000201 and Memory size: 9648\n",
      "Episode 99\tAverage Score: 0.16\tScore: 0.4351785281517853\n",
      "Epsilon: 0.9760900000000203 and Memory size: 9744\n",
      "Episode 100\tAverage Score: 0.16\tScore: 0.38049008367427933\n",
      "Epsilon: 0.9758800000000205 and Memory size: 9840\n",
      "Episode 100\tAverage Score: 0.16\n",
      "Episode 101\tAverage Score: 0.16\tScore: 0.4766892963089049\n",
      "Epsilon: 0.9756400000000207 and Memory size: 9936\n",
      "Episode 102\tAverage Score: 0.16\tScore: 0.24816865807709595\n",
      "Epsilon: 0.9753700000000209 and Memory size: 10044\n",
      "Episode 103\tAverage Score: 0.16\tScore: 0.40126450984583545\n",
      "Epsilon: 0.9751000000000212 and Memory size: 10152\n",
      "Episode 104\tAverage Score: 0.17\tScore: 0.4876334994332865\n",
      "Epsilon: 0.9748600000000214 and Memory size: 10248\n",
      "Episode 105\tAverage Score: 0.17\tScore: 0.5448182837571949\n",
      "Epsilon: 0.9746200000000216 and Memory size: 10344\n",
      "Episode 106\tAverage Score: 0.17\tScore: 0.5726839610530684\n",
      "Epsilon: 0.9743500000000218 and Memory size: 10464\n",
      "Episode 107\tAverage Score: 0.18\tScore: 0.6028000510608157\n",
      "Epsilon: 0.974080000000022 and Memory size: 10584\n",
      "Episode 108\tAverage Score: 0.17\tScore: -0.4630589122728755\n",
      "Epsilon: 0.972970000000023 and Memory size: 11052\n",
      "Episode 109\tAverage Score: 0.17\tScore: 0.4185305576150616\n",
      "Epsilon: 0.9727300000000232 and Memory size: 11148\n",
      "Episode 110\tAverage Score: 0.18\tScore: 0.42126362258568406\n",
      "Epsilon: 0.9724900000000234 and Memory size: 11244\n",
      "Episode 111\tAverage Score: 0.19\tScore: 0.39126708963885903\n",
      "Epsilon: 0.9722200000000236 and Memory size: 11352\n",
      "Episode 112\tAverage Score: 0.19\tScore: 0.2673660102300346\n",
      "Epsilon: 0.9719800000000238 and Memory size: 11460\n",
      "Episode 113\tAverage Score: 0.19\tScore: 0.43882556445896626\n",
      "Epsilon: 0.9716800000000241 and Memory size: 11580\n",
      "Episode 114\tAverage Score: 0.20\tScore: 0.34746624312053126\n",
      "Epsilon: 0.9714100000000243 and Memory size: 11688\n",
      "Episode 115\tAverage Score: 0.20\tScore: 0.3143154961677889\n",
      "Epsilon: 0.9710200000000246 and Memory size: 11844\n",
      "Episode 116\tAverage Score: 0.21\tScore: 0.20825218440343937\n",
      "Epsilon: 0.9707800000000248 and Memory size: 11940\n",
      "Episode 117\tAverage Score: 0.21\tScore: 0.5255021145955349\n",
      "Epsilon: 0.9705100000000251 and Memory size: 12048\n",
      "Episode 118\tAverage Score: 0.22\tScore: 0.44686558993998915\n",
      "Epsilon: 0.9702700000000253 and Memory size: 12144\n",
      "Episode 119\tAverage Score: 0.22\tScore: 0.4754505913006142\n",
      "Epsilon: 0.9700300000000255 and Memory size: 12240\n",
      "Episode 120\tAverage Score: 0.22\tScore: 0.5335280358946571\n",
      "Epsilon: 0.9697600000000257 and Memory size: 12348\n",
      "Episode 121\tAverage Score: 0.23\tScore: 0.43904019783561427\n",
      "Epsilon: 0.9695200000000259 and Memory size: 12444\n",
      "Episode 122\tAverage Score: 0.23\tScore: 0.3046478599620362\n",
      "Epsilon: 0.9693100000000261 and Memory size: 12540\n",
      "Episode 123\tAverage Score: 0.23\tScore: 0.34909530670847744\n",
      "Epsilon: 0.9690400000000263 and Memory size: 12648\n",
      "Episode 124\tAverage Score: 0.23\tScore: 0.4746800573775545\n",
      "Epsilon: 0.9688000000000265 and Memory size: 12744\n",
      "Episode 125\tAverage Score: 0.24\tScore: 0.36483701252533746\n",
      "Epsilon: 0.9685900000000267 and Memory size: 12840\n",
      "Episode 126\tAverage Score: 0.24\tScore: 0.38467343787973124\n",
      "Epsilon: 0.9683200000000269 and Memory size: 12948\n",
      "Episode 127\tAverage Score: 0.24\tScore: 0.38116711284965277\n",
      "Epsilon: 0.9680500000000272 and Memory size: 13056\n",
      "Episode 128\tAverage Score: 0.25\tScore: 0.48286463917853933\n",
      "Epsilon: 0.9678100000000274 and Memory size: 13152\n",
      "Episode 129\tAverage Score: 0.25\tScore: 0.4761353019469728\n",
      "Epsilon: 0.9675700000000276 and Memory size: 13248\n",
      "Episode 130\tAverage Score: 0.26\tScore: 0.47035613656044006\n",
      "Epsilon: 0.9673300000000278 and Memory size: 13344\n",
      "Episode 131\tAverage Score: 0.26\tScore: 0.3244658522695924\n",
      "Epsilon: 0.967090000000028 and Memory size: 13440\n",
      "Episode 132\tAverage Score: 0.27\tScore: 0.34654548748706776\n",
      "Epsilon: 0.9668500000000282 and Memory size: 13536\n",
      "Episode 133\tAverage Score: 0.27\tScore: 0.31097400293219835\n",
      "Epsilon: 0.9666400000000284 and Memory size: 13632\n",
      "Episode 134\tAverage Score: 0.28\tScore: 0.23504961491562426\n",
      "Epsilon: 0.9664000000000286 and Memory size: 13728\n",
      "Episode 135\tAverage Score: 0.28\tScore: 0.45634394387404126\n",
      "Epsilon: 0.9661600000000288 and Memory size: 13824\n",
      "Episode 136\tAverage Score: 0.29\tScore: 0.48973012253797304\n",
      "Epsilon: 0.965920000000029 and Memory size: 13920\n",
      "Episode 137\tAverage Score: 0.30\tScore: 0.5207653598239025\n",
      "Epsilon: 0.9656800000000292 and Memory size: 14016\n",
      "Episode 138\tAverage Score: 0.31\tScore: 0.6039952113060281\n",
      "Epsilon: 0.9654400000000294 and Memory size: 14112\n",
      "Episode 139\tAverage Score: 0.31\tScore: 0.47570696477002156\n",
      "Epsilon: 0.9652300000000296 and Memory size: 14208\n",
      "Episode 140\tAverage Score: 0.32\tScore: 0.5268446603634706\n",
      "Epsilon: 0.9649900000000298 and Memory size: 14304\n",
      "Episode 141\tAverage Score: 0.33\tScore: 0.4881063013647993\n",
      "Epsilon: 0.96478000000003 and Memory size: 14400\n",
      "Episode 142\tAverage Score: 0.33\tScore: 0.3909165289563437\n",
      "Epsilon: 0.9645400000000302 and Memory size: 14496\n",
      "Episode 143\tAverage Score: 0.34\tScore: 0.49114937203315395\n",
      "Epsilon: 0.9643300000000303 and Memory size: 14592\n",
      "Episode 144\tAverage Score: 0.35\tScore: 0.5597775919595733\n",
      "Epsilon: 0.9640900000000305 and Memory size: 14688\n",
      "Episode 145\tAverage Score: 0.35\tScore: 0.5913889090685794\n",
      "Epsilon: 0.9638500000000307 and Memory size: 14784\n",
      "Episode 146\tAverage Score: 0.35\tScore: 0.43381536883922917\n",
      "Epsilon: 0.963580000000031 and Memory size: 14892\n",
      "Episode 147\tAverage Score: 0.36\tScore: 0.583528241297851\n",
      "Epsilon: 0.9633400000000312 and Memory size: 14988\n",
      "Episode 148\tAverage Score: 0.36\tScore: 0.49344549460026127\n",
      "Epsilon: 0.9630700000000314 and Memory size: 15096\n",
      "Episode 149\tAverage Score: 0.37\tScore: 0.5612958723989626\n",
      "Epsilon: 0.9628300000000316 and Memory size: 15192\n",
      "Episode 150\tAverage Score: 0.37\tScore: 0.48685234765677404\n",
      "Epsilon: 0.9626200000000318 and Memory size: 15288\n",
      "Episode 151\tAverage Score: 0.38\tScore: 0.4954059262527153\n",
      "Epsilon: 0.962350000000032 and Memory size: 15396\n",
      "Episode 152\tAverage Score: 0.38\tScore: 0.47161867485071224\n",
      "Epsilon: 0.9620800000000322 and Memory size: 15504\n",
      "Episode 153\tAverage Score: 0.39\tScore: 0.5858415161880354\n",
      "Epsilon: 0.9618100000000325 and Memory size: 15612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 154\tAverage Score: 0.40\tScore: 0.5187004868251582\n",
      "Epsilon: 0.9615700000000327 and Memory size: 15708\n",
      "Episode 155\tAverage Score: 0.40\tScore: 0.47130816895514727\n",
      "Epsilon: 0.9613600000000329 and Memory size: 15804\n",
      "Episode 156\tAverage Score: 0.41\tScore: 0.3261973278907438\n",
      "Epsilon: 0.9611200000000331 and Memory size: 15900\n",
      "Episode 157\tAverage Score: 0.41\tScore: 0.5553020748775452\n",
      "Epsilon: 0.9608800000000333 and Memory size: 15996\n",
      "Episode 158\tAverage Score: 0.42\tScore: 0.5357728901241595\n",
      "Epsilon: 0.9606700000000334 and Memory size: 16092\n",
      "Episode 159\tAverage Score: 0.42\tScore: 0.3333418579616894\n",
      "Epsilon: 0.9603100000000337 and Memory size: 16236\n",
      "Episode 160\tAverage Score: 0.42\tScore: 0.42649092447633546\n",
      "Epsilon: 0.960070000000034 and Memory size: 16332\n",
      "Episode 161\tAverage Score: 0.42\tScore: 0.32369874860160053\n",
      "Epsilon: 0.9598600000000341 and Memory size: 16428\n",
      "Episode 162\tAverage Score: 0.43\tScore: 0.6199631257137904\n",
      "Epsilon: 0.9596200000000343 and Memory size: 16524\n",
      "Episode 163\tAverage Score: 0.43\tScore: 0.3549813787297656\n",
      "Epsilon: 0.9593500000000346 and Memory size: 16632\n",
      "Episode 164\tAverage Score: 0.43\tScore: 0.5938194931174318\n",
      "Epsilon: 0.9591100000000348 and Memory size: 16728\n",
      "Episode 165\tAverage Score: 0.44\tScore: 0.42584876731658977\n",
      "Epsilon: 0.958870000000035 and Memory size: 16824\n",
      "Episode 166\tAverage Score: 0.44\tScore: 0.38944740407168865\n",
      "Epsilon: 0.9586300000000352 and Memory size: 16920\n",
      "Episode 167\tAverage Score: 0.44\tScore: 0.37091911013703793\n",
      "Epsilon: 0.9583000000000355 and Memory size: 17064\n",
      "Episode 168\tAverage Score: 0.43\tScore: 0.32974166655912995\n",
      "Epsilon: 0.9580300000000357 and Memory size: 17172\n",
      "Episode 169\tAverage Score: 0.44\tScore: 0.6412139575307568\n",
      "Epsilon: 0.9577900000000359 and Memory size: 17268\n",
      "Episode 170\tAverage Score: 0.44\tScore: 0.6856134329767277\n",
      "Epsilon: 0.9575200000000361 and Memory size: 17376\n",
      "Episode 171\tAverage Score: 0.44\tScore: 0.5822442894180616\n",
      "Epsilon: 0.9572800000000363 and Memory size: 17472\n",
      "Episode 172\tAverage Score: 0.44\tScore: 0.514510163726906\n",
      "Epsilon: 0.9570400000000365 and Memory size: 17568\n",
      "Episode 173\tAverage Score: 0.44\tScore: 0.8229679592574636\n",
      "Epsilon: 0.9566500000000369 and Memory size: 17748\n",
      "Episode 174\tAverage Score: 0.44\tScore: 0.6496607018634677\n",
      "Epsilon: 0.9564100000000371 and Memory size: 17844\n",
      "Episode 175\tAverage Score: 0.45\tScore: 0.44903267834646005\n",
      "Epsilon: 0.9561400000000373 and Memory size: 17952\n",
      "Episode 176\tAverage Score: 0.45\tScore: 0.6081640526341895\n",
      "Epsilon: 0.9559300000000375 and Memory size: 18048\n",
      "Episode 177\tAverage Score: 0.45\tScore: 0.4407445211351539\n",
      "Epsilon: 0.9557500000000376 and Memory size: 18144\n",
      "Episode 178\tAverage Score: 0.45\tScore: 0.8954756744982054\n",
      "Epsilon: 0.9553900000000379 and Memory size: 18300\n",
      "Episode 179\tAverage Score: 0.45\tScore: 0.42607722043370205\n",
      "Epsilon: 0.9551500000000381 and Memory size: 18408\n",
      "Episode 180\tAverage Score: 0.45\tScore: 0.3737939779336254\n",
      "Epsilon: 0.9549100000000383 and Memory size: 18504\n",
      "Episode 181\tAverage Score: 0.45\tScore: 0.521220087694625\n",
      "Epsilon: 0.9547300000000385 and Memory size: 18600\n",
      "Episode 182\tAverage Score: 0.46\tScore: 0.867489319993183\n",
      "Epsilon: 0.9544900000000387 and Memory size: 18708\n",
      "Episode 183\tAverage Score: 0.46\tScore: 0.5956344271156316\n",
      "Epsilon: 0.9542200000000389 and Memory size: 18816\n",
      "Episode 184\tAverage Score: 0.46\tScore: 0.5249586905119941\n",
      "Epsilon: 0.9538000000000393 and Memory size: 18996\n",
      "Episode 185\tAverage Score: 0.46\tScore: 0.6878552439933022\n",
      "Epsilon: 0.9533500000000397 and Memory size: 19176\n",
      "Episode 186\tAverage Score: 0.46\tScore: 0.5229637476149946\n",
      "Epsilon: 0.9528100000000401 and Memory size: 19404\n",
      "Episode 187\tAverage Score: 0.47\tScore: 1.0648116076287504\n",
      "Epsilon: 0.9521500000000407 and Memory size: 19680\n",
      "Episode 188\tAverage Score: 0.47\tScore: 0.5036083872740468\n",
      "Epsilon: 0.9519100000000409 and Memory size: 19776\n",
      "Episode 189\tAverage Score: 0.47\tScore: 0.6729475723889967\n",
      "Epsilon: 0.9516400000000411 and Memory size: 19884\n",
      "Episode 190\tAverage Score: 0.48\tScore: 0.9536174922638262\n",
      "Epsilon: 0.9512200000000415 and Memory size: 20064\n",
      "Episode 191\tAverage Score: 0.48\tScore: 0.8245594900411864\n",
      "Epsilon: 0.9509800000000417 and Memory size: 20172\n",
      "Episode 192\tAverage Score: 0.48\tScore: 0.6818779425229877\n",
      "Epsilon: 0.9507100000000419 and Memory size: 20280\n",
      "Episode 193\tAverage Score: 0.49\tScore: 0.6670476920747509\n",
      "Epsilon: 0.9504400000000421 and Memory size: 20388\n",
      "Episode 194\tAverage Score: 0.49\tScore: 0.6938962240237743\n",
      "Epsilon: 0.9502000000000423 and Memory size: 20484\n",
      "Episode 195\tAverage Score: 0.49\tScore: 0.7487765331364548\n",
      "Epsilon: 0.9499900000000425 and Memory size: 20580\n",
      "Episode 196\tAverage Score: 0.49\tScore: 0.7100927971769124\n",
      "Epsilon: 0.9497800000000427 and Memory size: 20676\n",
      "Episode 197\tAverage Score: 0.50\tScore: 0.7694845124303052\n",
      "Epsilon: 0.949390000000043 and Memory size: 20832\n",
      "Episode 198\tAverage Score: 0.50\tScore: 0.661281218752265\n",
      "Epsilon: 0.9491500000000432 and Memory size: 20928\n",
      "Episode 199\tAverage Score: 0.50\tScore: 0.7431803747701148\n",
      "Epsilon: 0.9487600000000436 and Memory size: 21096\n",
      "Episode 200\tAverage Score: 0.51\tScore: 0.711022576627632\n",
      "Epsilon: 0.9485500000000437 and Memory size: 21192\n",
      "Episode 200\tAverage Score: 0.51\n",
      "Episode 201\tAverage Score: 0.51\tScore: 0.6603732301155105\n",
      "Epsilon: 0.948310000000044 and Memory size: 21288\n",
      "Episode 202\tAverage Score: 0.51\tScore: 0.8719256278903534\n",
      "Epsilon: 0.9479800000000442 and Memory size: 21432\n",
      "Episode 203\tAverage Score: 0.52\tScore: 0.659856497698153\n",
      "Epsilon: 0.9477400000000444 and Memory size: 21528\n",
      "Episode 204\tAverage Score: 0.52\tScore: 0.6769398323570689\n",
      "Epsilon: 0.9474700000000447 and Memory size: 21636\n",
      "Episode 205\tAverage Score: 0.52\tScore: 0.679983017539295\n",
      "Epsilon: 0.9472300000000449 and Memory size: 21732\n",
      "Episode 206\tAverage Score: 0.52\tScore: 0.7484558788128197\n",
      "Epsilon: 0.947080000000045 and Memory size: 21828\n",
      "Episode 207\tAverage Score: 0.52\tScore: 0.7182468195290616\n",
      "Epsilon: 0.9468700000000452 and Memory size: 21924\n",
      "Episode 208\tAverage Score: 0.53\tScore: 0.6313060191071903\n",
      "Epsilon: 0.9466300000000454 and Memory size: 22020\n",
      "Episode 209\tAverage Score: 0.54\tScore: 0.844490740642262\n",
      "Epsilon: 0.9462100000000457 and Memory size: 22188\n",
      "Episode 210\tAverage Score: 0.54\tScore: 0.865967969604147\n",
      "Epsilon: 0.945850000000046 and Memory size: 22344\n",
      "Episode 211\tAverage Score: 0.55\tScore: 0.7526282630084703\n",
      "Epsilon: 0.9456100000000462 and Memory size: 22440\n",
      "Episode 212\tAverage Score: 0.55\tScore: 0.7518753760959953\n",
      "Epsilon: 0.9454000000000464 and Memory size: 22536\n",
      "Episode 213\tAverage Score: 0.55\tScore: 0.745063827567113\n",
      "Epsilon: 0.9451300000000467 and Memory size: 22644\n",
      "Episode 214\tAverage Score: 0.56\tScore: 0.6962331707278887\n",
      "Epsilon: 0.9448900000000469 and Memory size: 22752\n",
      "Episode 215\tAverage Score: 0.56\tScore: 0.6742460445578521\n",
      "Epsilon: 0.9446500000000471 and Memory size: 22848\n",
      "Episode 216\tAverage Score: 0.56\tScore: 0.5764812067694342\n",
      "Epsilon: 0.9444100000000473 and Memory size: 22944\n",
      "Episode 217\tAverage Score: 0.57\tScore: 0.7517295788663129\n",
      "Epsilon: 0.9440200000000476 and Memory size: 23100\n",
      "Episode 218\tAverage Score: 0.57\tScore: 0.7018845583079383\n",
      "Epsilon: 0.9437800000000478 and Memory size: 23196\n",
      "Episode 219\tAverage Score: 0.57\tScore: 0.6775284220154086\n",
      "Epsilon: 0.9434800000000481 and Memory size: 23316\n",
      "Episode 220\tAverage Score: 0.57\tScore: 0.6522290998448929\n",
      "Epsilon: 0.9432400000000483 and Memory size: 23412\n",
      "Episode 221\tAverage Score: 0.58\tScore: 0.8062750461200873\n",
      "Epsilon: 0.9429100000000485 and Memory size: 23556\n",
      "Episode 222\tAverage Score: 0.58\tScore: 0.48047855713715154\n",
      "Epsilon: 0.9424600000000489 and Memory size: 23760\n",
      "Episode 223\tAverage Score: 0.58\tScore: 0.7821223857269312\n",
      "Epsilon: 0.9421900000000492 and Memory size: 23868\n",
      "Episode 224\tAverage Score: 0.58\tScore: 0.628355426248163\n",
      "Epsilon: 0.9419200000000494 and Memory size: 23976\n",
      "Episode 225\tAverage Score: 0.59\tScore: 0.6021000923356041\n",
      "Epsilon: 0.9417100000000496 and Memory size: 24072\n",
      "Episode 226\tAverage Score: 0.59\tScore: 0.7263846232090145\n",
      "Epsilon: 0.9415000000000497 and Memory size: 24168\n",
      "Episode 227\tAverage Score: 0.59\tScore: 0.6238720257921765\n",
      "Epsilon: 0.94126000000005 and Memory size: 24264\n",
      "Episode 228\tAverage Score: 0.59\tScore: 0.7011256443026165\n",
      "Epsilon: 0.9410500000000501 and Memory size: 24360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 229\tAverage Score: 0.60\tScore: 0.6479684360480557\n",
      "Epsilon: 0.9408400000000503 and Memory size: 24456\n",
      "Episode 230\tAverage Score: 0.60\tScore: 0.5927503335600098\n",
      "Epsilon: 0.9406000000000505 and Memory size: 24552\n",
      "Episode 231\tAverage Score: 0.60\tScore: 0.5556832108801851\n",
      "Epsilon: 0.9403300000000507 and Memory size: 24660\n",
      "Episode 232\tAverage Score: 0.60\tScore: 0.36138102400582284\n",
      "Epsilon: 0.9399400000000511 and Memory size: 24816\n",
      "Episode 233\tAverage Score: 0.60\tScore: 0.494425270628805\n",
      "Epsilon: 0.9397000000000513 and Memory size: 24912\n",
      "Episode 234\tAverage Score: 0.61\tScore: 0.7053811653361967\n",
      "Epsilon: 0.9394600000000515 and Memory size: 25008\n",
      "Episode 235\tAverage Score: 0.61\tScore: 0.7238927496752391\n",
      "Epsilon: 0.9391300000000518 and Memory size: 25152\n",
      "Episode 236\tAverage Score: 0.61\tScore: 0.9127250210537264\n",
      "Epsilon: 0.938800000000052 and Memory size: 25284\n",
      "Episode 237\tAverage Score: 0.61\tScore: 0.6559078589004154\n",
      "Epsilon: 0.9385300000000523 and Memory size: 25392\n",
      "Episode 238\tAverage Score: 0.62\tScore: 0.7486044564284384\n",
      "Epsilon: 0.9380800000000526 and Memory size: 25584\n",
      "Episode 239\tAverage Score: 0.62\tScore: 0.8268273725019147\n",
      "Epsilon: 0.9376000000000531 and Memory size: 25800\n",
      "Episode 240\tAverage Score: 0.62\tScore: 0.9163188091091191\n",
      "Epsilon: 0.9372400000000534 and Memory size: 25944\n",
      "Episode 241\tAverage Score: 0.63\tScore: 1.074476977926679\n",
      "Epsilon: 0.9368800000000537 and Memory size: 26088\n",
      "Episode 242\tAverage Score: 0.63\tScore: 0.939431504967312\n",
      "Epsilon: 0.9366700000000538 and Memory size: 26208\n",
      "Episode 243\tAverage Score: 0.63\tScore: 0.26385493731747073\n",
      "Epsilon: 0.936460000000054 and Memory size: 26292\n",
      "Episode 244\tAverage Score: 0.63\tScore: 0.3709408347106849\n",
      "Epsilon: 0.9361600000000543 and Memory size: 26412\n",
      "Episode 245\tAverage Score: 0.63\tScore: 0.37856290528240305\n",
      "Epsilon: 0.9359500000000545 and Memory size: 26496\n",
      "Episode 246\tAverage Score: 0.62\tScore: 0.07772774171705048\n",
      "Epsilon: 0.9357400000000546 and Memory size: 26580\n",
      "Episode 247\tAverage Score: 0.62\tScore: 0.22775796389517686\n",
      "Epsilon: 0.9355600000000548 and Memory size: 26664\n",
      "Episode 248\tAverage Score: 0.62\tScore: 0.29738781733127934\n",
      "Epsilon: 0.935380000000055 and Memory size: 26748\n",
      "Episode 249\tAverage Score: 0.62\tScore: 0.2726378334142889\n",
      "Epsilon: 0.9351700000000551 and Memory size: 26832\n",
      "Episode 250\tAverage Score: 0.61\tScore: 0.2632079321580629\n",
      "Epsilon: 0.9349600000000553 and Memory size: 26916\n",
      "Episode 251\tAverage Score: 0.61\tScore: 0.2846581613412127\n",
      "Epsilon: 0.9347500000000555 and Memory size: 27000\n",
      "Episode 252\tAverage Score: 0.61\tScore: 0.23759549600072205\n",
      "Epsilon: 0.9344800000000557 and Memory size: 27108\n",
      "Episode 253\tAverage Score: 0.61\tScore: 0.3848925790904711\n",
      "Epsilon: 0.9342700000000559 and Memory size: 27192\n",
      "Episode 254\tAverage Score: 0.60\tScore: 0.2099339101308336\n",
      "Epsilon: 0.9340600000000561 and Memory size: 27276\n",
      "Episode 255\tAverage Score: 0.60\tScore: 0.4128414332711448\n",
      "Epsilon: 0.9338500000000562 and Memory size: 27360\n",
      "Episode 256\tAverage Score: 0.60\tScore: 0.2656151072975869\n",
      "Epsilon: 0.9336700000000564 and Memory size: 27444\n",
      "Episode 257\tAverage Score: 0.60\tScore: 0.2147387263054649\n",
      "Epsilon: 0.9334600000000566 and Memory size: 27528\n",
      "Episode 258\tAverage Score: 0.60\tScore: 0.5110256177916502\n",
      "Epsilon: 0.9331000000000569 and Memory size: 27684\n",
      "Episode 259\tAverage Score: 0.60\tScore: 0.40620400896295905\n",
      "Epsilon: 0.9328600000000571 and Memory size: 27780\n",
      "Episode 260\tAverage Score: 0.60\tScore: 0.2993351249024272\n",
      "Epsilon: 0.9326800000000572 and Memory size: 27864\n",
      "Episode 261\tAverage Score: 0.60\tScore: -0.008298919769003987\n",
      "Epsilon: 0.9325000000000574 and Memory size: 27948\n",
      "Episode 262\tAverage Score: 0.59\tScore: 0.34872515755705535\n",
      "Epsilon: 0.9322900000000576 and Memory size: 28032\n",
      "Episode 263\tAverage Score: 0.59\tScore: 0.38357505085878074\n",
      "Epsilon: 0.9320800000000578 and Memory size: 28116\n",
      "Episode 264\tAverage Score: 0.59\tScore: 0.2702417182736099\n",
      "Epsilon: 0.9318700000000579 and Memory size: 28200\n",
      "Episode 265\tAverage Score: 0.59\tScore: 0.4262219616988053\n",
      "Epsilon: 0.9316300000000581 and Memory size: 28296\n",
      "Episode 266\tAverage Score: 0.59\tScore: 0.25640369377409417\n",
      "Epsilon: 0.9314200000000583 and Memory size: 28380\n",
      "Episode 267\tAverage Score: 0.59\tScore: 0.2142812015954405\n",
      "Epsilon: 0.9312100000000585 and Memory size: 28464\n",
      "Episode 268\tAverage Score: 0.59\tScore: 0.2935935252656539\n",
      "Epsilon: 0.9310000000000587 and Memory size: 28548\n",
      "Episode 269\tAverage Score: 0.58\tScore: 0.32456461995995295\n",
      "Epsilon: 0.9308500000000588 and Memory size: 28632\n",
      "Episode 270\tAverage Score: 0.58\tScore: 0.25862723612226546\n",
      "Epsilon: 0.930640000000059 and Memory size: 28716\n",
      "Episode 271\tAverage Score: 0.58\tScore: 0.34181473264470696\n",
      "Epsilon: 0.9304600000000591 and Memory size: 28800\n",
      "Episode 272\tAverage Score: 0.57\tScore: 0.27034959142717224\n",
      "Epsilon: 0.9302200000000593 and Memory size: 28896\n",
      "Episode 273\tAverage Score: 0.57\tScore: 0.33959708467591554\n",
      "Epsilon: 0.9300400000000595 and Memory size: 28980\n",
      "Episode 274\tAverage Score: 0.57\tScore: 0.40339031590459246\n",
      "Epsilon: 0.9298300000000597 and Memory size: 29064\n",
      "Episode 275\tAverage Score: 0.56\tScore: 0.2018160104441146\n",
      "Epsilon: 0.9296200000000598 and Memory size: 29148\n",
      "Episode 276\tAverage Score: 0.56\tScore: 0.2582626805718367\n",
      "Epsilon: 0.92944000000006 and Memory size: 29232\n",
      "Episode 277\tAverage Score: 0.56\tScore: 0.36182312477224815\n",
      "Epsilon: 0.9292300000000602 and Memory size: 29328\n",
      "Episode 278\tAverage Score: 0.55\tScore: 0.29737982297471416\n",
      "Epsilon: 0.9290200000000604 and Memory size: 29424\n",
      "Episode 279\tAverage Score: 0.55\tScore: 0.31859698848954093\n",
      "Epsilon: 0.9287800000000606 and Memory size: 29520\n",
      "Episode 280\tAverage Score: 0.55\tScore: 0.4086092247162014\n",
      "Epsilon: 0.9285700000000607 and Memory size: 29604\n",
      "Episode 281\tAverage Score: 0.55\tScore: 0.44587204527730745\n",
      "Epsilon: 0.9284200000000609 and Memory size: 29688\n",
      "Episode 282\tAverage Score: 0.55\tScore: 0.36820236469308537\n",
      "Epsilon: 0.928210000000061 and Memory size: 29772\n",
      "Episode 283\tAverage Score: 0.55\tScore: 0.46245353979369\n",
      "Epsilon: 0.9280000000000612 and Memory size: 29856\n",
      "Episode 284\tAverage Score: 0.54\tScore: 0.33313256374094635\n",
      "Epsilon: 0.9277900000000614 and Memory size: 29940\n",
      "Episode 285\tAverage Score: 0.54\tScore: 0.4337042428087443\n",
      "Epsilon: 0.9275500000000616 and Memory size: 30036\n",
      "Episode 286\tAverage Score: 0.54\tScore: 0.42889288677057874\n",
      "Epsilon: 0.9273700000000618 and Memory size: 30120\n",
      "Episode 287\tAverage Score: 0.53\tScore: 0.47663936701913673\n",
      "Epsilon: 0.927130000000062 and Memory size: 30216\n",
      "Episode 288\tAverage Score: 0.53\tScore: 0.35968714913663763\n",
      "Epsilon: 0.9269200000000621 and Memory size: 30300\n",
      "Episode 289\tAverage Score: 0.53\tScore: 0.46612548956181854\n",
      "Epsilon: 0.9267400000000623 and Memory size: 30384\n",
      "Episode 290\tAverage Score: 0.52\tScore: 0.3192123080758999\n",
      "Epsilon: 0.9265600000000624 and Memory size: 30468\n",
      "Episode 291\tAverage Score: 0.52\tScore: 0.352115718415007\n",
      "Epsilon: 0.9263500000000626 and Memory size: 30552\n",
      "Episode 292\tAverage Score: 0.51\tScore: 0.14762297637450197\n",
      "Epsilon: 0.9262000000000628 and Memory size: 30636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-70e2a5b2c3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9a494b848a01>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m         \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                         \u001b[0;31m# get rewards (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/openai/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    335\u001b[0m                         \u001b[0mvector_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_agent\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_brains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_action_space_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                     \u001b[0mvector_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/openai/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m_flatten\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
