{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.elu(self.fc1(state))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.elu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4 #3e-5 #1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4 #3e-5 #1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY_actor = 0.0 #3e-4 #0        # L2 weight decay\n",
    "WEIGHT_DECAY_critic = 0.0 #1e-6 #0        # L2 weight decay\n",
    "\n",
    "#to decay exploration as it learns\n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY_critic)\n",
    "\n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed) #single agent only\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed) #both singe and multiple agent\n",
    "        self.eps = EPS_START\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "        # Make sure target is initialized with the same weight as the source (found on slack to make big difference)\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        #Experience from each agent is separately saved (so it works for single or multi agent environment)\n",
    "        #This works because each agent is operating in a separate/independent environment\n",
    "        for a in range(self.num_agents):\n",
    "            self.memory.add(states[a], actions[a], rewards[a], next_states[a], dones[a])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        #add noise according to epsilon probability\n",
    "        if add_noise and (np.random.random() < self.eps):\n",
    "            actions += self.noise.sample()\n",
    "            #update the exploration parameter\n",
    "            self.eps -= EPS_DECAY\n",
    "            if self.eps < EPS_END:\n",
    "                self.eps = EPS_END\n",
    "            #self.noise.reset() #not sure if need to do this here\n",
    "\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0) #clip the gradient for the critic network (Udacity hint)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "    ## from slack - Since you're using DDPG, @gregoriomezquita mentioned that \n",
    "    ## initializing the weights of the target networks to be the same as those \n",
    "    ## of the live networks seemed to make a huge difference\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(source_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], action_size=brain.vector_action_space_size, \n",
    "              num_agents=env_info.vector_observations.shape[0],  random_seed=0)\n",
    "\n",
    "\n",
    "#for single and multiple agents\n",
    "def ddpg(n_episodes=2000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_list = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                   # get the current states (for all agents)\n",
    "        agent.reset() #reset the agent OU Noise\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            agent.step(states, actions, rewards, next_states, dones) #train the agent\n",
    "            \n",
    "            # Extra Learning per time step (since generating so much experience at each step)\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                for _ in range(3):\n",
    "                    experiences = agent.memory.sample()\n",
    "                    agent.learn(experiences, GAMMA)\n",
    "            \n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            #print('Total score (averaged over agents) this episode: {}'.format(np.mean(score)))\n",
    "        \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        scores_list.append(np.mean(scores))\n",
    "        \n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.mean(scores)))\n",
    "        print('Epsilon: {} and Memory size: {}'.format(agent.eps, len(agent.memory)))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) > 30 and len(scores_deque) >= 100:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/openai/lib/python3.6/site-packages/ipykernel_launcher.py:117: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.92\tScore: 0.9184999794699251\n",
      "Epsilon: 0.970540000000025 and Memory size: 20020\n",
      "Episode 2\tAverage Score: 1.00\tScore: 1.0749999759718776\n",
      "Epsilon: 0.9416500000000496 and Memory size: 40040\n",
      "Episode 3\tAverage Score: 1.16\tScore: 1.499499966483563\n",
      "Epsilon: 0.9138100000000733 and Memory size: 60060\n",
      "Episode 4\tAverage Score: 1.34\tScore: 1.8839999578893185\n",
      "Epsilon: 0.8868100000000962 and Memory size: 80080\n",
      "Episode 5\tAverage Score: 1.65\tScore: 2.8844999355264007\n",
      "Epsilon: 0.8605900000001185 and Memory size: 100000\n",
      "Episode 6\tAverage Score: 1.85\tScore: 2.8284999367780985\n",
      "Epsilon: 0.8344900000001407 and Memory size: 100000\n",
      "Episode 7\tAverage Score: 2.09\tScore: 3.5314999210648237\n",
      "Epsilon: 0.8092000000001622 and Memory size: 100000\n",
      "Episode 8\tAverage Score: 2.24\tScore: 3.3244999256916343\n",
      "Epsilon: 0.7854700000001824 and Memory size: 100000\n",
      "Episode 9\tAverage Score: 2.52\tScore: 4.744499893952161\n",
      "Epsilon: 0.762460000000202 and Memory size: 100000\n",
      "Episode 10\tAverage Score: 2.77\tScore: 5.025499887671321\n",
      "Epsilon: 0.7397200000002213 and Memory size: 100000\n",
      "Episode 11\tAverage Score: 3.05\tScore: 5.8369998695328835\n",
      "Epsilon: 0.7175800000002401 and Memory size: 100000\n",
      "Episode 12\tAverage Score: 3.36\tScore: 6.82349984748289\n",
      "Epsilon: 0.6957700000002587 and Memory size: 100000\n",
      "Episode 13\tAverage Score: 3.83\tScore: 9.428499789256602\n",
      "Epsilon: 0.674200000000277 and Memory size: 100000\n",
      "Episode 14\tAverage Score: 4.20\tScore: 9.032499798107892\n",
      "Epsilon: 0.6541600000002941 and Memory size: 100000\n",
      "Episode 15\tAverage Score: 4.57\tScore: 9.693999783322216\n",
      "Epsilon: 0.6348700000003105 and Memory size: 100000\n",
      "Episode 16\tAverage Score: 5.06\tScore: 12.494999720714986\n",
      "Epsilon: 0.6165100000003261 and Memory size: 100000\n",
      "Episode 17\tAverage Score: 5.49\tScore: 12.24849972622469\n",
      "Epsilon: 0.5986000000003413 and Memory size: 100000\n",
      "Episode 18\tAverage Score: 6.09\tScore: 16.25999963656068\n",
      "Epsilon: 0.5815000000003558 and Memory size: 100000\n",
      "Episode 19\tAverage Score: 7.03\tScore: 24.06549946209416\n",
      "Epsilon: 0.5641900000003706 and Memory size: 100000\n",
      "Episode 20\tAverage Score: 8.11\tScore: 28.676499359030277\n",
      "Epsilon: 0.5469400000003852 and Memory size: 100000\n",
      "Episode 21\tAverage Score: 9.30\tScore: 33.04049926148728\n",
      "Epsilon: 0.5310700000003987 and Memory size: 100000\n",
      "Episode 22\tAverage Score: 10.49\tScore: 35.48799920678139\n",
      "Epsilon: 0.514330000000413 and Memory size: 100000\n",
      "Episode 23\tAverage Score: 11.68\tScore: 37.94849915178493\n",
      "Epsilon: 0.4999300000004252 and Memory size: 100000\n",
      "Episode 24\tAverage Score: 12.82\tScore: 38.879499130975454\n",
      "Epsilon: 0.48598000000043706 and Memory size: 100000\n",
      "Episode 25\tAverage Score: 13.87\tScore: 39.20049912380055\n",
      "Epsilon: 0.47092000000044987 and Memory size: 100000\n",
      "Episode 26\tAverage Score: 14.84\tScore: 38.91899913009256\n",
      "Epsilon: 0.45613000000046244 and Memory size: 100000\n",
      "Episode 27\tAverage Score: 15.73\tScore: 38.943499129544946\n",
      "Epsilon: 0.44317000000047346 and Memory size: 100000\n",
      "Episode 28\tAverage Score: 16.57\tScore: 39.38899911958724\n",
      "Epsilon: 0.4305400000004842 and Memory size: 100000\n",
      "Episode 29\tAverage Score: 17.36\tScore: 39.328999120928344\n",
      "Epsilon: 0.4180900000004948 and Memory size: 100000\n",
      "Episode 30\tAverage Score: 18.05\tScore: 38.20949914595112\n",
      "Epsilon: 0.406090000000505 and Memory size: 100000\n",
      "Episode 31\tAverage Score: 18.70\tScore: 38.17099914681167\n",
      "Epsilon: 0.3945400000005148 and Memory size: 100000\n",
      "Episode 32\tAverage Score: 19.33\tScore: 38.62799913659692\n",
      "Epsilon: 0.3834100000005243 and Memory size: 100000\n",
      "Episode 33\tAverage Score: 19.92\tScore: 38.87199913114309\n",
      "Epsilon: 0.37228000000053374 and Memory size: 100000\n",
      "Episode 34\tAverage Score: 20.49\tScore: 39.33549912078306\n",
      "Epsilon: 0.36112000000054323 and Memory size: 100000\n",
      "Episode 35\tAverage Score: 21.02\tScore: 39.08649912634864\n",
      "Epsilon: 0.35062000000055216 and Memory size: 100000\n",
      "Episode 36\tAverage Score: 21.52\tScore: 38.9984991283156\n",
      "Epsilon: 0.34006000000056114 and Memory size: 100000\n",
      "Episode 37\tAverage Score: 21.98\tScore: 38.66299913581461\n",
      "Epsilon: 0.3304600000005693 and Memory size: 100000\n",
      "Episode 38\tAverage Score: 22.43\tScore: 39.013499127980324\n",
      "Epsilon: 0.3204400000005778 and Memory size: 100000\n",
      "Episode 39\tAverage Score: 22.85\tScore: 38.64449913622811\n",
      "Epsilon: 0.3110500000005858 and Memory size: 100000\n",
      "Episode 40\tAverage Score: 23.21\tScore: 37.21249916823581\n",
      "Epsilon: 0.30184000000059363 and Memory size: 100000\n",
      "Episode 41\tAverage Score: 23.59\tScore: 38.81399913243949\n",
      "Epsilon: 0.29230000000060175 and Memory size: 100000\n",
      "Episode 42\tAverage Score: 23.93\tScore: 37.80899915490299\n",
      "Epsilon: 0.28336000000060935 and Memory size: 100000\n",
      "Episode 43\tAverage Score: 24.27\tScore: 38.539499138575046\n",
      "Epsilon: 0.2750500000006164 and Memory size: 100000\n",
      "Episode 44\tAverage Score: 24.57\tScore: 37.6889991575852\n",
      "Epsilon: 0.2663500000006238 and Memory size: 100000\n",
      "Episode 45\tAverage Score: 24.86\tScore: 37.49799916185439\n",
      "Epsilon: 0.2583400000006306 and Memory size: 100000\n",
      "Episode 46\tAverage Score: 25.16\tScore: 38.59699913728982\n",
      "Epsilon: 0.2501500000006376 and Memory size: 100000\n",
      "Episode 47\tAverage Score: 25.45\tScore: 39.178999124281106\n",
      "Epsilon: 0.24289000000063718 and Memory size: 100000\n",
      "Episode 48\tAverage Score: 25.73\tScore: 38.54399913847446\n",
      "Epsilon: 0.23566000000063664 and Memory size: 100000\n",
      "Episode 49\tAverage Score: 25.97\tScore: 37.504999161697924\n",
      "Epsilon: 0.2286100000006361 and Memory size: 100000\n",
      "Episode 50\tAverage Score: 26.21\tScore: 38.048999149538574\n",
      "Epsilon: 0.22222000000063563 and Memory size: 100000\n",
      "Episode 51\tAverage Score: 26.47\tScore: 39.41999911889434\n",
      "Epsilon: 0.21574000000063515 and Memory size: 100000\n",
      "Episode 52\tAverage Score: 26.72\tScore: 39.46399911791086\n",
      "Epsilon: 0.20944000000063467 and Memory size: 100000\n",
      "Episode 53\tAverage Score: 26.93\tScore: 38.13549914760515\n",
      "Epsilon: 0.2032900000006342 and Memory size: 100000\n",
      "Episode 54\tAverage Score: 27.12\tScore: 37.17099916916341\n",
      "Epsilon: 0.19681000000063373 and Memory size: 100000\n",
      "Episode 55\tAverage Score: 27.33\tScore: 38.57349913781509\n",
      "Epsilon: 0.1909900000006333 and Memory size: 100000\n",
      "Episode 56\tAverage Score: 27.53\tScore: 38.2504991450347\n",
      "Epsilon: 0.18514000000063285 and Memory size: 100000\n",
      "Episode 57\tAverage Score: 27.72\tScore: 38.2794991443865\n",
      "Epsilon: 0.17986000000063246 and Memory size: 100000\n",
      "Episode 58\tAverage Score: 27.90\tScore: 38.51049913922325\n",
      "Epsilon: 0.17455000000063206 and Memory size: 100000\n",
      "Episode 59\tAverage Score: 28.09\tScore: 38.94299912955612\n",
      "Epsilon: 0.16945000000063168 and Memory size: 100000\n",
      "Episode 60\tAverage Score: 28.26\tScore: 38.4099991414696\n",
      "Epsilon: 0.16384000000063126 and Memory size: 100000\n",
      "Episode 61\tAverage Score: 28.42\tScore: 38.17899914663285\n",
      "Epsilon: 0.15925000000063091 and Memory size: 100000\n",
      "Episode 62\tAverage Score: 28.54\tScore: 35.3804992091842\n",
      "Epsilon: 0.15460000000063057 and Memory size: 100000\n",
      "Episode 63\tAverage Score: 28.68\tScore: 37.46699916254729\n",
      "Epsilon: 0.1498900000006302 and Memory size: 100000\n",
      "Episode 64\tAverage Score: 28.82\tScore: 37.76649915585294\n",
      "Epsilon: 0.14545000000062988 and Memory size: 100000\n",
      "Episode 65\tAverage Score: 28.95\tScore: 37.095499170850964\n",
      "Epsilon: 0.14104000000062955 and Memory size: 100000\n",
      "Episode 66\tAverage Score: 29.05\tScore: 35.840999198891225\n",
      "Epsilon: 0.13705000000062925 and Memory size: 100000\n",
      "Episode 67\tAverage Score: 29.16\tScore: 36.145999192073944\n",
      "Epsilon: 0.13258000000062892 and Memory size: 100000\n",
      "Episode 68\tAverage Score: 29.26\tScore: 35.97349919592962\n",
      "Epsilon: 0.1283800000006286 and Memory size: 100000\n",
      "Episode 69\tAverage Score: 29.39\tScore: 38.11499914806336\n",
      "Epsilon: 0.12523000000062837 and Memory size: 100000\n",
      "Episode 70\tAverage Score: 29.50\tScore: 37.42449916349724\n",
      "Epsilon: 0.12181000000062811 and Memory size: 100000\n",
      "Episode 71\tAverage Score: 29.63\tScore: 38.828999132104215\n",
      "Epsilon: 0.11800000000062782 and Memory size: 100000\n",
      "Episode 72\tAverage Score: 29.76\tScore: 38.590999137423935\n",
      "Epsilon: 0.11410000000062753 and Memory size: 100000\n",
      "Episode 73\tAverage Score: 29.85\tScore: 36.39449918651953\n",
      "Epsilon: 0.10981000000062721 and Memory size: 100000\n",
      "Episode 74\tAverage Score: 29.93\tScore: 36.326999188028275\n",
      "Epsilon: 0.10627000000062695 and Memory size: 100000\n",
      "Episode 75\tAverage Score: 30.01\tScore: 35.60699920412153\n",
      "Epsilon: 0.10279000000062669 and Memory size: 100000\n",
      "Episode 76\tAverage Score: 30.10\tScore: 36.71149917943403\n",
      "Epsilon: 0.09958000000062645 and Memory size: 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77\tAverage Score: 30.19\tScore: 36.816499177087096\n",
      "Epsilon: 0.0963400000006262 and Memory size: 100000\n",
      "Episode 78\tAverage Score: 30.29\tScore: 38.02449915008619\n",
      "Epsilon: 0.09307000000062596 and Memory size: 100000\n",
      "Episode 79\tAverage Score: 30.39\tScore: 38.20799914598465\n",
      "Epsilon: 0.09025000000062575 and Memory size: 100000\n",
      "Episode 80\tAverage Score: 30.46\tScore: 36.35199918746948\n",
      "Epsilon: 0.08758000000062555 and Memory size: 100000\n",
      "Episode 81\tAverage Score: 30.56\tScore: 38.5819991376251\n",
      "Epsilon: 0.0856300000006254 and Memory size: 100000\n",
      "Episode 82\tAverage Score: 30.67\tScore: 39.077999126538636\n",
      "Epsilon: 0.08305000000062521 and Memory size: 100000\n",
      "Episode 83\tAverage Score: 30.75\tScore: 37.74849915625528\n",
      "Epsilon: 0.08071000000062503 and Memory size: 100000\n",
      "Episode 84\tAverage Score: 30.82\tScore: 36.27049918929115\n",
      "Epsilon: 0.07828000000062485 and Memory size: 100000\n",
      "Episode 85\tAverage Score: 30.89\tScore: 37.11799917034805\n",
      "Epsilon: 0.07591000000062467 and Memory size: 100000\n",
      "Episode 86\tAverage Score: 30.95\tScore: 36.10949919288978\n",
      "Epsilon: 0.07330000000062448 and Memory size: 100000\n",
      "Episode 87\tAverage Score: 31.02\tScore: 37.05749917170033\n",
      "Epsilon: 0.07123000000062432 and Memory size: 100000\n",
      "Episode 88\tAverage Score: 31.06\tScore: 34.59399922676384\n",
      "Epsilon: 0.06883000000062414 and Memory size: 100000\n",
      "Episode 89\tAverage Score: 31.11\tScore: 35.034999216906726\n",
      "Epsilon: 0.06646000000062396 and Memory size: 100000\n",
      "Episode 90\tAverage Score: 31.16\tScore: 36.19449919098988\n",
      "Epsilon: 0.0642100000006238 and Memory size: 100000\n",
      "Episode 91\tAverage Score: 31.24\tScore: 38.44349914072082\n",
      "Epsilon: 0.06241000000062366 and Memory size: 100000\n",
      "Episode 92\tAverage Score: 31.31\tScore: 37.68249915773049\n",
      "Epsilon: 0.060880000000623546 and Memory size: 100000\n",
      "Episode 93\tAverage Score: 31.35\tScore: 35.179999213665724\n",
      "Epsilon: 0.058870000000623396 and Memory size: 100000\n",
      "Episode 94\tAverage Score: 31.41\tScore: 36.385999186709526\n",
      "Epsilon: 0.057250000000623275 and Memory size: 100000\n",
      "Episode 95\tAverage Score: 31.47\tScore: 37.75849915603176\n",
      "Epsilon: 0.055510000000623144 and Memory size: 100000\n",
      "Episode 96\tAverage Score: 31.54\tScore: 37.67749915784225\n",
      "Epsilon: 0.05410000000062304 and Memory size: 100000\n",
      "Episode 97\tAverage Score: 31.59\tScore: 36.85899917613715\n",
      "Epsilon: 0.052450000000622915 and Memory size: 100000\n",
      "Episode 98\tAverage Score: 31.66\tScore: 38.35299914274365\n",
      "Epsilon: 0.05047000000062277 and Memory size: 100000\n",
      "Episode 99\tAverage Score: 31.73\tScore: 38.56449913801625\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 100\tAverage Score: 31.78\tScore: 36.52999918349087\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 100\tAverage Score: 31.78\n",
      "Episode 100\tAverage Score: 31.78\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 36.64749918086454\n",
      "Episode 2: 36.05899919401854\n",
      "Episode 3: 37.56199916042387\n",
      "Mean score is:  36.756165845102316\n"
     ]
    }
   ],
   "source": [
    "#Inference\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], action_size=brain.vector_action_space_size, \n",
    "              num_agents=env_info.vector_observations.shape[0],  random_seed=0)\n",
    "#Since the model is trained on gpu, need to load all gpu tensors to cpu:\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "agent.eps = 0.05\n",
    "scores_list = []\n",
    "def ddpg_inference(n_episodes=3):\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                   # get the current states (for all agents)\n",
    "        agent.reset() #reset the agent OU Noise\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get rewards (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Episode {}: {}'.format(i_episode, np.mean(scores)))\n",
    "        scores_list.append(np.mean(scores))\n",
    "    print('Mean score is: ', np.mean(np.array(scores_list)))\n",
    "\n",
    "ddpg_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
